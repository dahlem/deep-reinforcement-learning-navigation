{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/VisualBanana.app\"`\n",
    "- **Windows** (x86): `\"path/to/VisualBanana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/VisualBanana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/VisualBanana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/VisualBanana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/VisualBanana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/VisualBanana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `VisualBanana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"VisualBanana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:unityagents:The true file name is Banana\n",
      "DEBUG:unityagents:This is the launch string /Users/ddahlem/githubs/deep-reinforcement-learning-navigation/Banana.app/Contents/MacOS/mac_banana\n",
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Reinforcement Learning!\n",
    "\n",
    "In order to train your own agent to solve the environment we take few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- In this coding environment, you will not be able to watch the agent while it is training.  However, **_after training the agent_**, you can download the saved model weights to watch the agent on your own machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Loop\n",
    "\n",
    "The following `train` function encapsulates the training loop. It accepts an `agent` and a dictionary of parameters for the individual components, including replay buffer, policy, and agent.\n",
    "\n",
    "The dictionary of parameters take the following form:\n",
    "```python\n",
    "params = {\n",
    "    'name': 'DDQN',              # name of the experiment\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'max_t': 1000,               # maximum length of an episode\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 1.0,            # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,             # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 0.995           # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'agent_params': {\n",
    "        'alpha': 0.4,                # alpha power value for the prioritized replay buffer sampling\n",
    "        'gamma': 0.99,               # discount factor\n",
    "        'tau': 0.001,                # mixing rate soft-update of target parameters\n",
    "        'lr': 0.0005,                # learning rate\n",
    "        'update_every': 4,           # update every n-th step\n",
    "        'network_type': QNetwork,    # network architecture\n",
    "        'network_params': {\n",
    "            'state_size': state_size,    # size of the state space\n",
    "            'action_size': action_size,  # size of the action space\n",
    "            'seed': 6453,                # seed of the network architecture\n",
    "            'hidden_layers': [64, 32],   # hidden layer neurons\n",
    "            'dropout': 0.05\n",
    "        },\n",
    "        'experience_params': {\n",
    "            'seed': 184,                 # seed for the experience replay buffer\n",
    "            'buffer_size': 1000,         # size of the replay buffer\n",
    "            'batch_size': 64             # batch size sampled from the replay buffer\n",
    "        },\n",
    "    },\n",
    "    'maxlen': 100,               # sliding window size of recent scores\n",
    "    'beta_start': 0.4,           # starting value for the beta value of prioritized replay sampling\n",
    "    'brain_name': brain_name     # the brain name of the unity environment\n",
    "}\n",
    "```\n",
    "\n",
    "The dictionary of parameters introduces scoping of the configuration parameters with respect to the component being configured. One consideration is to afford each component its own random number seed to avoid spurious correlation introduced by a single shared seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.value.train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Replay Buffer\n",
    "\n",
    "There are two replay buffer implementations, `UniformExperienceBuffer` and `PrioritizedExperienceBuffer`. Both inherit from an abstract base class `ReplayBuffer` that declares the abstract methods:\n",
    " - `add()`: add an experience to the buffer of a fixed and confirgurable size\n",
    " - `sample()`: sample from the buffer\n",
    " - `ready()`: check whether the batch size is smaller than the current buffer size\n",
    " \n",
    "The `UniformExperienceBuffer` implements a uniform sampling strategy that makes no distinction between the individual experiences, while the `PrioritizedExperienceBuffer` implements the strategy set out in the original paper by [Schaul et al.](https://arxiv.org/abs/1511.05952). The experiences are replayed according to the magnitude of the temporal difference error, thus given the agent the opportunity to learn from more meaningful transitions.\n",
    "\n",
    "The probabilities of sampling from the experience buffer are defined as:\n",
    "\n",
    "$$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$$\n",
    "\n",
    "where $p_i$ is the prioritisation of the $i$-th experience and $alpha \\in [0; 1]$ is a smoothing parameter. If $alpha$ is equal to zero than this conforms to the uniform sampling strategy and the closer to 1 the stronger the weight towards to the true probabilities.\n",
    "\n",
    "The priorities, $p_i$, are computed as the magnitude of the TD error plus an additional small positive number for numeric stability in case the error is very close to zero, i.e., $p_i=|\\delta_i| + \\epsilon$.\n",
    "\n",
    "We further need to introduce importance-sampling weights, because of the associated bias in the prioritized experience replay:\n",
    "\n",
    "$$w_i = (\\frac{1}{N} * \\frac{1}{P(i)})^\\beta$$\n",
    "\n",
    "where $\\beta$ is annealed to 1 towards the end of training. This parameter interacts with the $\\alpha$ value in that sampling more aggressively ($\\alpha = 1$) is corrected more strongly in the IS weights ($\\beta = 1$). The weights, $w_i$, are also scaled by $\\frac{1}{\\max(w)}$.\n",
    "\n",
    "### 3.3 Double Q-Networks\n",
    "\n",
    "As for the deep reinforcement algorithms, this solution uses two approaches. The first one is based on double deep Q-network function approximators in reinforcement learning settings, which has been found to address the general problem of value overestimation in traditional DQN architectures [Hasselt, Guez, Silver](https://arxiv.org/abs/1509.06461). This is accomplished by decoupling the determination of the greedy policy from the evaluation of its target value. The solution works as follows:\n",
    "\n",
    "- Use the DQN, with the online weights $\\theta$, to compute the greedy action given the next state\n",
    "- Use the target DQN, with weights $\\theta^{\\prime}$, to compute the value of taking that action in the next state\n",
    "\n",
    "$$Q(s, a, \\theta^{\\prime}) = r(s, a) + \\gamma * Q(s^{\\prime}, argmax_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}, \\theta), \\theta^{\\prime})$$\n",
    "\n",
    "At the end of the learning process, we can then perform a soft-update to switch the weights:\n",
    "\n",
    "$$\\theta^{\\prime} = \\tau * \\theta + (1 - \\tau) * \\theta^{\\prime}$$\n",
    "\n",
    "where $\\tau$ is a mixing factor of the two weight vectors eminating from the online and target network respectively.\n",
    "\n",
    "For the DQN, the following architecture is implemented:\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Linear-1                [-1, 1, 128]          4,736\n",
    "           Dropout-2                [-1, 1, 128]              0\n",
    "            Linear-3                [-1, 1, 64]           8,192\n",
    "           Dropout-4                [-1, 1, 64]               0\n",
    "            Linear-5                 [-1, 1, 4]             256\n",
    "================================================================\n",
    "Total params: 13,184\n",
    "Trainable params: 13,184\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "```\n",
    "\n",
    "### 3.4 Dueling Double Q-Network\n",
    "\n",
    "The underlying mechanism of Dueling Double Q-Networks (DDQN) rely on the decomposition of Q-value function into [Wang et al.](https://arxiv.org/abs/1511.06581):\n",
    "\n",
    "$$Q(s, a) = A(s, a) + V(s)$$\n",
    "\n",
    "where \n",
    "\n",
    "- $A(s, a)$ is an advantage function of an action $a$ in state $s$\n",
    "- $V(s)$ is the value-function given state $s$.\n",
    "\n",
    "Decomposing the Q-value function this way has the advantage of being able to learn the value of a state without having to also include the effect of the actions given the states. This is useful when the action values do not need to be computed. However, the ability to decompose $Q(s, a)$ this way does imply that for every unique Q-value we have unique advantage and value functions respectively. To address this, we subtract the mean of the advantage function over all actions from the chosen action:\n",
    "\n",
    "$$Q(s, a, \\theta, \\alpha, \\beta) = V(s, \\theta, \\alpha) + A(s, a, \\theta, \\beta) - \\frac{1}{B}\\sum_{a^{\\prime}}A(s, a^{\\prime}, \\theta, \\beta)$$\n",
    "\n",
    "The DDQN architecture is also fixed as follows:\n",
    "\n",
    "```\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Linear-1                [-1, 1, 128]           4,736\n",
    "           Dropout-2                [-1, 1, 128]               0\n",
    "            Linear-2                [-1, 1, 64]            8,192\n",
    "           Dropout-3                [-1, 1, 64]                0\n",
    "      Value-Linear-4.1              [-1, 1, 512]          32,768\n",
    "     Value-Dropout-5.1              [-1, 1, 512]               0\n",
    "      Value-Linear-6.1              [-1, 1, 1]               512\n",
    "     Value-Dropout-7.1              [-1, 1, 1]                 0\n",
    "  Advantage-Linear-4.2              [-1, 1, 512]          32,768\n",
    " Advantage-Dropout-5.2              [-1, 1, 512]               0\n",
    "  Advantage-Linear-6.2              [-1, 1, 4]             2,048\n",
    " Advantage-Dropout-7.2              [-1, 1, 4]                 0\n",
    "================================================================\n",
    "Total params: 81,024\n",
    "Trainable params: 81,024\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Both, the value and advantage fully connected layers, $\\alpha$ and $\\beta$ respectively, are separate streams off the first shared fully connected layers, $\\theta$, (all with ReLU activation and dropout)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Value-based Methods and Experiments\n",
    "This section covers 4 experiments without hyperparameter tuning. The two combinations of experience replay and network architectures are pitted against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving into the experiments, I'd like to highlight some of the dynamic parameters of this implementation, namely the epsilon-greedy policy ($\\epsilon$) and those involved in the prioritized replay ($\\alpha$ and $\\beta$) respectively. We fixed $\\alpha=0.4$. The other two parameters are scheduled along the episode number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:rl.value.policy:Parameter: {'eps_start': 1.0, 'eps_end': 0.01, 'eps_decay': 0.9975}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x117ae3f98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.font_manager:findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/Users/ddahlem/anaconda3/envs/drlnd/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rl.utils.policy import EpsilonGreedy\n",
    "\n",
    "params = {\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 1.0,            # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,             # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 0.9975           # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'beta_start': 0.4            # starting value for the beta value of prioritized replay sampling\n",
    "}\n",
    "\n",
    "n_episodes = params['episodes']\n",
    "beta_start = params['beta_start']\n",
    "policy = params['policy'](params['policy_params'])\n",
    "\n",
    "beta_schedule = lambda episode: min(1.0, beta_start + episode * (1.0 - beta_start) / n_episodes)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "for e in np.arange(n_episodes):\n",
    "    df = df.append(pd.DataFrame({'Episode': e, 'Beta': beta_schedule(e), 'Epsilon': policy.eps}, index=[0]), ignore_index=True)\n",
    "    policy.decay()\n",
    "\n",
    "df.plot(x='Episode', y=['Beta', 'Epsilon'], kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 DQN Agent with Uniform Experience Replay\n",
    "\n",
    "This experiment configures a Double Q-Network Agent with uniform experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:rl.value.agent:Parameter: {'alpha': 0.4, 'gamma': 0.99, 'tau': 0.001, 'lr': 5e-05, 'update_every': 4, 'network_type': <class 'rl.value.model.QNetwork'>, 'network_params': {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}, 'experience_params': {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}}\n",
      "DEBUG:rl.value.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}\n",
      "DEBUG:rl.value.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}\n",
      "DEBUG:rl.value.buffer:Parameter: {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}\n",
      "DEBUG:rl.value.policy:Parameter: {'eps_start': 1.0, 'eps_end': 0.01, 'eps_decay': 0.9975}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4\tAverage Score: 0.75"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f6f86a295380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQN_UER_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agent_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DDQN_UER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/value/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, params)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m# see if episode has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m                                \u001b[0;31m# update the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m                             \u001b[0;31m# roll over the state to next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/value/agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done, beta)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/value/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# ------------------- update target network ------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/value/agent.py\u001b[0m in \u001b[0;36msoft_update\u001b[0;34m(self, local_model, target_model)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlocal_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rl.value.agent import DDQN_UER_Agent\n",
    "from rl.value.model import QNetwork\n",
    "from rl.utils.policy import EpsilonGreedy\n",
    "\n",
    "params = {\n",
    "    'name': 'DDQN_UER',\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'max_t': 5000,               # maximum length of an episode\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 1.0,            # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,             # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 0.9975          # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'agent_params': {\n",
    "        'alpha': 0.4,                # alpha power value for the prioritized replay buffer sampling\n",
    "        'gamma': 0.99,               # discount factor\n",
    "        'tau': 0.001,                # mixing rate soft-update of target parameters\n",
    "        'lr': 0.00005,               # learning rate\n",
    "        'update_every': 4,           # update every n-th step\n",
    "        'network_type': QNetwork,    # network architecture\n",
    "        'network_params': {\n",
    "            'state_size': state_size,    # size of the state space\n",
    "            'action_size': action_size,  # size of the action space\n",
    "            'seed': 6453,                # seed of the network architecture\n",
    "            'hidden_layers': [128, 64],  # hidden layer neurons\n",
    "            'dropout': 0.30\n",
    "        },\n",
    "        'experience_params': {\n",
    "            'seed': 184,                 # seed for the experience replay buffer\n",
    "            'buffer_size': 1000,         # size of the replay buffer\n",
    "            'batch_size': 64             # batch size sampled from the replay buffer\n",
    "        },\n",
    "    },\n",
    "    'maxlen': 100,               # sliding window size of recent scores\n",
    "    'beta_start': 0.4,           # starting value for the beta value of prioritized replay sampling\n",
    "    'brain_name': brain_name     # the brain name of the unity environment\n",
    "}\n",
    "\n",
    "agent = DDQN_UER_Agent(params=params.get('agent_params'))\n",
    "scores = train(agent=agent, env=env, params=params)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'DDQN_UER': scores})\n",
    "df.to_csv('results/DDQN_UER_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 DDQN Agent with Uniform Experience Replay\n",
    "\n",
    "This experiment configures a Dueling Double Q-Network agent with uniform experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:rl.agent:Parameter: {'alpha': 0.4, 'gamma': 0.99, 'tau': 0.001, 'lr': 5e-05, 'update_every': 4, 'network_type': <class 'rl.model.DuelingQNetwork'>, 'network_params': {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.01}, 'experience_params': {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}}\n",
      "DEBUG:rl.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.01}\n",
      "DEBUG:rl.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.01}\n",
      "DEBUG:rl.buffer:Parameter: {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}\n",
      "DEBUG:rl.policy:Parameter: {'eps_start': 1.0, 'eps_end': 0.01, 'eps_decay': 0.9975}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.14\n",
      "Episode 200\tAverage Score: 1.35\n",
      "Episode 300\tAverage Score: 3.27\n",
      "Episode 400\tAverage Score: 5.44\n",
      "Episode 500\tAverage Score: 7.64\n",
      "Episode 600\tAverage Score: 8.01\n",
      "Episode 700\tAverage Score: 10.62\n",
      "Episode 800\tAverage Score: 11.58\n",
      "Episode 900\tAverage Score: 12.01\n",
      "Episode 969\tAverage Score: 13.04\n",
      "Environment solved in 869 episodes!\tAverage Score: 13.04\n",
      "Episode 970\tAverage Score: 13.01\n",
      "Environment solved in 870 episodes!\tAverage Score: 13.01\n",
      "Episode 971\tAverage Score: 13.01\n",
      "Environment solved in 871 episodes!\tAverage Score: 13.01\n",
      "Episode 972\tAverage Score: 13.06\n",
      "Environment solved in 872 episodes!\tAverage Score: 13.06\n",
      "Episode 981\tAverage Score: 13.02\n",
      "Environment solved in 881 episodes!\tAverage Score: 13.02\n",
      "Episode 983\tAverage Score: 13.11\n",
      "Environment solved in 883 episodes!\tAverage Score: 13.11\n",
      "Episode 984\tAverage Score: 13.11\n",
      "Environment solved in 884 episodes!\tAverage Score: 13.11\n",
      "Episode 985\tAverage Score: 13.07\n",
      "Environment solved in 885 episodes!\tAverage Score: 13.07\n",
      "Episode 986\tAverage Score: 13.09\n",
      "Environment solved in 886 episodes!\tAverage Score: 13.09\n",
      "Episode 987\tAverage Score: 13.06\n",
      "Environment solved in 887 episodes!\tAverage Score: 13.06\n",
      "Episode 988\tAverage Score: 13.08\n",
      "Environment solved in 888 episodes!\tAverage Score: 13.08\n",
      "Episode 989\tAverage Score: 13.11\n",
      "Environment solved in 889 episodes!\tAverage Score: 13.11\n",
      "Episode 990\tAverage Score: 13.15\n",
      "Environment solved in 890 episodes!\tAverage Score: 13.15\n",
      "Episode 991\tAverage Score: 13.17\n",
      "Environment solved in 891 episodes!\tAverage Score: 13.17\n",
      "Episode 992\tAverage Score: 13.20\n",
      "Environment solved in 892 episodes!\tAverage Score: 13.20\n",
      "Episode 993\tAverage Score: 13.14\n",
      "Environment solved in 893 episodes!\tAverage Score: 13.14\n",
      "Episode 994\tAverage Score: 13.17\n",
      "Environment solved in 894 episodes!\tAverage Score: 13.17\n",
      "Episode 995\tAverage Score: 13.09\n",
      "Environment solved in 895 episodes!\tAverage Score: 13.09\n",
      "Episode 996\tAverage Score: 13.05\n",
      "Environment solved in 896 episodes!\tAverage Score: 13.05\n",
      "Episode 997\tAverage Score: 13.15\n",
      "Environment solved in 897 episodes!\tAverage Score: 13.15\n",
      "Episode 998\tAverage Score: 13.13\n",
      "Environment solved in 898 episodes!\tAverage Score: 13.13\n",
      "Episode 999\tAverage Score: 13.14\n",
      "Environment solved in 899 episodes!\tAverage Score: 13.14\n",
      "Episode 1000\tAverage Score: 13.15\n",
      "\n",
      "Environment solved in 900 episodes!\tAverage Score: 13.15\n",
      "Episode 1001\tAverage Score: 13.18\n",
      "Environment solved in 901 episodes!\tAverage Score: 13.18\n",
      "Episode 1002\tAverage Score: 13.17\n",
      "Environment solved in 902 episodes!\tAverage Score: 13.17\n",
      "Episode 1003\tAverage Score: 13.10\n",
      "Environment solved in 903 episodes!\tAverage Score: 13.10\n",
      "Episode 1004\tAverage Score: 13.14\n",
      "Environment solved in 904 episodes!\tAverage Score: 13.14\n",
      "Episode 1005\tAverage Score: 13.20\n",
      "Environment solved in 905 episodes!\tAverage Score: 13.20\n",
      "Episode 1006\tAverage Score: 13.23\n",
      "Environment solved in 906 episodes!\tAverage Score: 13.23\n",
      "Episode 1007\tAverage Score: 13.27\n",
      "Environment solved in 907 episodes!\tAverage Score: 13.27\n",
      "Episode 1008\tAverage Score: 13.31\n",
      "Environment solved in 908 episodes!\tAverage Score: 13.31\n",
      "Episode 1009\tAverage Score: 13.30\n",
      "Environment solved in 909 episodes!\tAverage Score: 13.30\n",
      "Episode 1010\tAverage Score: 13.32\n",
      "Environment solved in 910 episodes!\tAverage Score: 13.32\n",
      "Episode 1011\tAverage Score: 13.26\n",
      "Environment solved in 911 episodes!\tAverage Score: 13.26\n",
      "Episode 1012\tAverage Score: 13.27\n",
      "Environment solved in 912 episodes!\tAverage Score: 13.27\n",
      "Episode 1013\tAverage Score: 13.29\n",
      "Environment solved in 913 episodes!\tAverage Score: 13.29\n",
      "Episode 1014\tAverage Score: 13.36\n",
      "Environment solved in 914 episodes!\tAverage Score: 13.36\n",
      "Episode 1015\tAverage Score: 13.41\n",
      "Environment solved in 915 episodes!\tAverage Score: 13.41\n",
      "Episode 1016\tAverage Score: 13.38\n",
      "Environment solved in 916 episodes!\tAverage Score: 13.38\n",
      "Episode 1017\tAverage Score: 13.39\n",
      "Environment solved in 917 episodes!\tAverage Score: 13.39\n",
      "Episode 1018\tAverage Score: 13.34\n",
      "Environment solved in 918 episodes!\tAverage Score: 13.34\n",
      "Episode 1019\tAverage Score: 13.39\n",
      "Environment solved in 919 episodes!\tAverage Score: 13.39\n",
      "Episode 1020\tAverage Score: 13.44\n",
      "Environment solved in 920 episodes!\tAverage Score: 13.44\n",
      "Episode 1021\tAverage Score: 13.42\n",
      "Environment solved in 921 episodes!\tAverage Score: 13.42\n",
      "Episode 1022\tAverage Score: 13.48\n",
      "Environment solved in 922 episodes!\tAverage Score: 13.48\n",
      "Episode 1023\tAverage Score: 13.46\n",
      "Environment solved in 923 episodes!\tAverage Score: 13.46\n",
      "Episode 1024\tAverage Score: 13.50\n",
      "Environment solved in 924 episodes!\tAverage Score: 13.50\n",
      "Episode 1025\tAverage Score: 13.48\n",
      "Environment solved in 925 episodes!\tAverage Score: 13.48\n",
      "Episode 1026\tAverage Score: 13.46\n",
      "Environment solved in 926 episodes!\tAverage Score: 13.46\n",
      "Episode 1027\tAverage Score: 13.43\n",
      "Environment solved in 927 episodes!\tAverage Score: 13.43\n",
      "Episode 1028\tAverage Score: 13.51\n",
      "Environment solved in 928 episodes!\tAverage Score: 13.51\n",
      "Episode 1029\tAverage Score: 13.51\n",
      "Environment solved in 929 episodes!\tAverage Score: 13.51\n",
      "Episode 1030\tAverage Score: 13.57\n",
      "Environment solved in 930 episodes!\tAverage Score: 13.57\n",
      "Episode 1031\tAverage Score: 13.62\n",
      "Environment solved in 931 episodes!\tAverage Score: 13.62\n",
      "Episode 1032\tAverage Score: 13.63\n",
      "Environment solved in 932 episodes!\tAverage Score: 13.63\n",
      "Episode 1033\tAverage Score: 13.62\n",
      "Environment solved in 933 episodes!\tAverage Score: 13.62\n",
      "Episode 1034\tAverage Score: 13.67\n",
      "Environment solved in 934 episodes!\tAverage Score: 13.67\n",
      "Episode 1035\tAverage Score: 13.65\n",
      "Environment solved in 935 episodes!\tAverage Score: 13.65\n",
      "Episode 1036\tAverage Score: 13.66\n",
      "Environment solved in 936 episodes!\tAverage Score: 13.66\n",
      "Episode 1037\tAverage Score: 13.74\n",
      "Environment solved in 937 episodes!\tAverage Score: 13.74\n",
      "Episode 1038\tAverage Score: 13.82\n",
      "Environment solved in 938 episodes!\tAverage Score: 13.82\n",
      "Episode 1039\tAverage Score: 13.83\n",
      "Environment solved in 939 episodes!\tAverage Score: 13.83\n",
      "Episode 1040\tAverage Score: 13.92\n",
      "Environment solved in 940 episodes!\tAverage Score: 13.92\n",
      "Episode 1041\tAverage Score: 13.91\n",
      "Environment solved in 941 episodes!\tAverage Score: 13.91\n",
      "Episode 1042\tAverage Score: 14.00\n",
      "Environment solved in 942 episodes!\tAverage Score: 14.00\n",
      "Episode 1043\tAverage Score: 14.02\n",
      "Environment solved in 943 episodes!\tAverage Score: 14.02\n",
      "Episode 1044\tAverage Score: 14.05\n",
      "Environment solved in 944 episodes!\tAverage Score: 14.05\n",
      "Episode 1045\tAverage Score: 14.08\n",
      "Environment solved in 945 episodes!\tAverage Score: 14.08\n",
      "Episode 1046\tAverage Score: 14.08\n",
      "Environment solved in 946 episodes!\tAverage Score: 14.08\n",
      "Episode 1047\tAverage Score: 14.07\n",
      "Environment solved in 947 episodes!\tAverage Score: 14.07\n",
      "Episode 1048\tAverage Score: 14.17\n",
      "Environment solved in 948 episodes!\tAverage Score: 14.17\n",
      "Episode 1049\tAverage Score: 14.24\n",
      "Environment solved in 949 episodes!\tAverage Score: 14.24\n",
      "Episode 1050\tAverage Score: 14.27\n",
      "Environment solved in 950 episodes!\tAverage Score: 14.27\n",
      "Episode 1051\tAverage Score: 14.32\n",
      "Environment solved in 951 episodes!\tAverage Score: 14.32\n",
      "Episode 1052\tAverage Score: 14.36\n",
      "Environment solved in 952 episodes!\tAverage Score: 14.36\n",
      "Episode 1053\tAverage Score: 14.42\n",
      "Environment solved in 953 episodes!\tAverage Score: 14.42\n",
      "Episode 1054\tAverage Score: 14.44\n",
      "Environment solved in 954 episodes!\tAverage Score: 14.44\n",
      "Episode 1055\tAverage Score: 14.48\n",
      "Environment solved in 955 episodes!\tAverage Score: 14.48\n",
      "Episode 1056\tAverage Score: 14.48\n",
      "Environment solved in 956 episodes!\tAverage Score: 14.48\n",
      "Episode 1057\tAverage Score: 14.50\n",
      "Environment solved in 957 episodes!\tAverage Score: 14.50\n",
      "Episode 1058\tAverage Score: 14.47\n",
      "Environment solved in 958 episodes!\tAverage Score: 14.47\n",
      "Episode 1059\tAverage Score: 14.47\n",
      "Environment solved in 959 episodes!\tAverage Score: 14.47\n",
      "Episode 1060\tAverage Score: 14.48\n",
      "Environment solved in 960 episodes!\tAverage Score: 14.48\n",
      "Episode 1061\tAverage Score: 14.49\n",
      "Environment solved in 961 episodes!\tAverage Score: 14.49\n",
      "Episode 1062\tAverage Score: 14.49\n",
      "Environment solved in 962 episodes!\tAverage Score: 14.49\n",
      "Episode 1063\tAverage Score: 14.52\n",
      "Environment solved in 963 episodes!\tAverage Score: 14.52\n",
      "Episode 1064\tAverage Score: 14.52\n",
      "Environment solved in 964 episodes!\tAverage Score: 14.52\n",
      "Episode 1065\tAverage Score: 14.44\n",
      "Environment solved in 965 episodes!\tAverage Score: 14.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1066\tAverage Score: 14.45\n",
      "Environment solved in 966 episodes!\tAverage Score: 14.45\n",
      "Episode 1067\tAverage Score: 14.43\n",
      "Environment solved in 967 episodes!\tAverage Score: 14.43\n",
      "Episode 1068\tAverage Score: 14.38\n",
      "Environment solved in 968 episodes!\tAverage Score: 14.38\n",
      "Episode 1069\tAverage Score: 14.38\n",
      "Environment solved in 969 episodes!\tAverage Score: 14.38\n",
      "Episode 1070\tAverage Score: 14.45\n",
      "Environment solved in 970 episodes!\tAverage Score: 14.45\n",
      "Episode 1071\tAverage Score: 14.42\n",
      "Environment solved in 971 episodes!\tAverage Score: 14.42\n",
      "Episode 1072\tAverage Score: 14.34\n",
      "Environment solved in 972 episodes!\tAverage Score: 14.34\n",
      "Episode 1073\tAverage Score: 14.41\n",
      "Environment solved in 973 episodes!\tAverage Score: 14.41\n",
      "Episode 1074\tAverage Score: 14.40\n",
      "Environment solved in 974 episodes!\tAverage Score: 14.40\n",
      "Episode 1075\tAverage Score: 14.49\n",
      "Environment solved in 975 episodes!\tAverage Score: 14.49\n",
      "Episode 1076\tAverage Score: 14.52\n",
      "Environment solved in 976 episodes!\tAverage Score: 14.52\n",
      "Episode 1077\tAverage Score: 14.55\n",
      "Environment solved in 977 episodes!\tAverage Score: 14.55\n",
      "Episode 1078\tAverage Score: 14.57\n",
      "Environment solved in 978 episodes!\tAverage Score: 14.57\n",
      "Episode 1079\tAverage Score: 14.62\n",
      "Environment solved in 979 episodes!\tAverage Score: 14.62\n",
      "Episode 1080\tAverage Score: 14.56\n",
      "Environment solved in 980 episodes!\tAverage Score: 14.56\n",
      "Episode 1081\tAverage Score: 14.45\n",
      "Environment solved in 981 episodes!\tAverage Score: 14.45\n",
      "Episode 1082\tAverage Score: 14.46\n",
      "Environment solved in 982 episodes!\tAverage Score: 14.46\n",
      "Episode 1083\tAverage Score: 14.43\n",
      "Environment solved in 983 episodes!\tAverage Score: 14.43\n",
      "Episode 1084\tAverage Score: 14.42\n",
      "Environment solved in 984 episodes!\tAverage Score: 14.42\n",
      "Episode 1085\tAverage Score: 14.46\n",
      "Environment solved in 985 episodes!\tAverage Score: 14.46\n",
      "Episode 1086\tAverage Score: 14.41\n",
      "Environment solved in 986 episodes!\tAverage Score: 14.41\n",
      "Episode 1087\tAverage Score: 14.47\n",
      "Environment solved in 987 episodes!\tAverage Score: 14.47\n",
      "Episode 1088\tAverage Score: 14.47\n",
      "Environment solved in 988 episodes!\tAverage Score: 14.47\n",
      "Episode 1089\tAverage Score: 14.45\n",
      "Environment solved in 989 episodes!\tAverage Score: 14.45\n",
      "Episode 1090\tAverage Score: 14.45\n",
      "Environment solved in 990 episodes!\tAverage Score: 14.45\n",
      "Episode 1091\tAverage Score: 14.44\n",
      "Environment solved in 991 episodes!\tAverage Score: 14.44\n",
      "Episode 1092\tAverage Score: 14.35\n",
      "Environment solved in 992 episodes!\tAverage Score: 14.35\n",
      "Episode 1093\tAverage Score: 14.37\n",
      "Environment solved in 993 episodes!\tAverage Score: 14.37\n",
      "Episode 1094\tAverage Score: 14.33\n",
      "Environment solved in 994 episodes!\tAverage Score: 14.33\n",
      "Episode 1095\tAverage Score: 14.38\n",
      "Environment solved in 995 episodes!\tAverage Score: 14.38\n",
      "Episode 1096\tAverage Score: 14.37\n",
      "Environment solved in 996 episodes!\tAverage Score: 14.37\n",
      "Episode 1097\tAverage Score: 14.24\n",
      "Environment solved in 997 episodes!\tAverage Score: 14.24\n",
      "Episode 1098\tAverage Score: 14.18\n",
      "Environment solved in 998 episodes!\tAverage Score: 14.18\n",
      "Episode 1099\tAverage Score: 14.12\n",
      "Environment solved in 999 episodes!\tAverage Score: 14.12\n",
      "Episode 1100\tAverage Score: 14.07\n",
      "\n",
      "Environment solved in 1000 episodes!\tAverage Score: 14.07\n",
      "Episode 1101\tAverage Score: 13.99\n",
      "Environment solved in 1001 episodes!\tAverage Score: 13.99\n",
      "Episode 1102\tAverage Score: 14.02\n",
      "Environment solved in 1002 episodes!\tAverage Score: 14.02\n",
      "Episode 1103\tAverage Score: 13.96\n",
      "Environment solved in 1003 episodes!\tAverage Score: 13.96\n",
      "Episode 1104\tAverage Score: 13.91\n",
      "Environment solved in 1004 episodes!\tAverage Score: 13.91\n",
      "Episode 1105\tAverage Score: 13.85\n",
      "Environment solved in 1005 episodes!\tAverage Score: 13.85\n",
      "Episode 1106\tAverage Score: 13.73\n",
      "Environment solved in 1006 episodes!\tAverage Score: 13.73\n",
      "Episode 1107\tAverage Score: 13.68\n",
      "Environment solved in 1007 episodes!\tAverage Score: 13.68\n",
      "Episode 1108\tAverage Score: 13.61\n",
      "Environment solved in 1008 episodes!\tAverage Score: 13.61\n",
      "Episode 1109\tAverage Score: 13.58\n",
      "Environment solved in 1009 episodes!\tAverage Score: 13.58\n",
      "Episode 1110\tAverage Score: 13.56\n",
      "Environment solved in 1010 episodes!\tAverage Score: 13.56\n",
      "Episode 1111\tAverage Score: 13.53\n",
      "Environment solved in 1011 episodes!\tAverage Score: 13.53\n",
      "Episode 1112\tAverage Score: 13.45\n",
      "Environment solved in 1012 episodes!\tAverage Score: 13.45\n",
      "Episode 1113\tAverage Score: 13.46\n",
      "Environment solved in 1013 episodes!\tAverage Score: 13.46\n",
      "Episode 1114\tAverage Score: 13.38\n",
      "Environment solved in 1014 episodes!\tAverage Score: 13.38\n",
      "Episode 1115\tAverage Score: 13.37\n",
      "Environment solved in 1015 episodes!\tAverage Score: 13.37\n",
      "Episode 1116\tAverage Score: 13.42\n",
      "Environment solved in 1016 episodes!\tAverage Score: 13.42\n",
      "Episode 1117\tAverage Score: 13.37\n",
      "Environment solved in 1017 episodes!\tAverage Score: 13.37\n",
      "Episode 1118\tAverage Score: 13.37\n",
      "Environment solved in 1018 episodes!\tAverage Score: 13.37\n",
      "Episode 1119\tAverage Score: 13.31\n",
      "Environment solved in 1019 episodes!\tAverage Score: 13.31\n",
      "Episode 1120\tAverage Score: 13.26\n",
      "Environment solved in 1020 episodes!\tAverage Score: 13.26\n",
      "Episode 1121\tAverage Score: 13.28\n",
      "Environment solved in 1021 episodes!\tAverage Score: 13.28\n",
      "Episode 1122\tAverage Score: 13.30\n",
      "Environment solved in 1022 episodes!\tAverage Score: 13.30\n",
      "Episode 1123\tAverage Score: 13.29\n",
      "Environment solved in 1023 episodes!\tAverage Score: 13.29\n",
      "Episode 1124\tAverage Score: 13.25\n",
      "Environment solved in 1024 episodes!\tAverage Score: 13.25\n",
      "Episode 1125\tAverage Score: 13.30\n",
      "Environment solved in 1025 episodes!\tAverage Score: 13.30\n",
      "Episode 1126\tAverage Score: 13.34\n",
      "Environment solved in 1026 episodes!\tAverage Score: 13.34\n",
      "Episode 1127\tAverage Score: 13.37\n",
      "Environment solved in 1027 episodes!\tAverage Score: 13.37\n",
      "Episode 1128\tAverage Score: 13.34\n",
      "Environment solved in 1028 episodes!\tAverage Score: 13.34\n",
      "Episode 1129\tAverage Score: 13.34\n",
      "Environment solved in 1029 episodes!\tAverage Score: 13.34\n",
      "Episode 1130\tAverage Score: 13.32\n",
      "Environment solved in 1030 episodes!\tAverage Score: 13.32\n",
      "Episode 1131\tAverage Score: 13.31\n",
      "Environment solved in 1031 episodes!\tAverage Score: 13.31\n",
      "Episode 1132\tAverage Score: 13.29\n",
      "Environment solved in 1032 episodes!\tAverage Score: 13.29\n",
      "Episode 1133\tAverage Score: 13.28\n",
      "Environment solved in 1033 episodes!\tAverage Score: 13.28\n",
      "Episode 1134\tAverage Score: 13.25\n",
      "Environment solved in 1034 episodes!\tAverage Score: 13.25\n",
      "Episode 1135\tAverage Score: 13.32\n",
      "Environment solved in 1035 episodes!\tAverage Score: 13.32\n",
      "Episode 1136\tAverage Score: 13.30\n",
      "Environment solved in 1036 episodes!\tAverage Score: 13.30\n",
      "Episode 1137\tAverage Score: 13.22\n",
      "Environment solved in 1037 episodes!\tAverage Score: 13.22\n",
      "Episode 1138\tAverage Score: 13.19\n",
      "Environment solved in 1038 episodes!\tAverage Score: 13.19\n",
      "Episode 1139\tAverage Score: 13.19\n",
      "Environment solved in 1039 episodes!\tAverage Score: 13.19\n",
      "Episode 1140\tAverage Score: 13.17\n",
      "Environment solved in 1040 episodes!\tAverage Score: 13.17\n",
      "Episode 1141\tAverage Score: 13.16\n",
      "Environment solved in 1041 episodes!\tAverage Score: 13.16\n",
      "Episode 1142\tAverage Score: 13.09\n",
      "Environment solved in 1042 episodes!\tAverage Score: 13.09\n",
      "Episode 1143\tAverage Score: 13.10\n",
      "Environment solved in 1043 episodes!\tAverage Score: 13.10\n",
      "Episode 1144\tAverage Score: 13.04\n",
      "Environment solved in 1044 episodes!\tAverage Score: 13.04\n",
      "Episode 1145\tAverage Score: 13.04\n",
      "Environment solved in 1045 episodes!\tAverage Score: 13.04\n",
      "Episode 1146\tAverage Score: 13.02\n",
      "Environment solved in 1046 episodes!\tAverage Score: 13.02\n",
      "Episode 1147\tAverage Score: 13.01\n",
      "Environment solved in 1047 episodes!\tAverage Score: 13.01\n",
      "Episode 1190\tAverage Score: 13.02\n",
      "Environment solved in 1090 episodes!\tAverage Score: 13.02\n",
      "Episode 1191\tAverage Score: 13.04\n",
      "Environment solved in 1091 episodes!\tAverage Score: 13.04\n",
      "Episode 1192\tAverage Score: 13.12\n",
      "Environment solved in 1092 episodes!\tAverage Score: 13.12\n",
      "Episode 1193\tAverage Score: 13.17\n",
      "Environment solved in 1093 episodes!\tAverage Score: 13.17\n",
      "Episode 1194\tAverage Score: 13.31\n",
      "Environment solved in 1094 episodes!\tAverage Score: 13.31\n",
      "Episode 1195\tAverage Score: 13.28\n",
      "Environment solved in 1095 episodes!\tAverage Score: 13.28\n",
      "Episode 1196\tAverage Score: 13.30\n",
      "Environment solved in 1096 episodes!\tAverage Score: 13.30\n",
      "Episode 1197\tAverage Score: 13.39\n",
      "Environment solved in 1097 episodes!\tAverage Score: 13.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1198\tAverage Score: 13.47\n",
      "Environment solved in 1098 episodes!\tAverage Score: 13.47\n",
      "Episode 1199\tAverage Score: 13.52\n",
      "Environment solved in 1099 episodes!\tAverage Score: 13.52\n",
      "Episode 1200\tAverage Score: 13.61\n",
      "\n",
      "Environment solved in 1100 episodes!\tAverage Score: 13.61\n",
      "Episode 1201\tAverage Score: 13.69\n",
      "Environment solved in 1101 episodes!\tAverage Score: 13.69\n",
      "Episode 1202\tAverage Score: 13.73\n",
      "Environment solved in 1102 episodes!\tAverage Score: 13.73\n",
      "Episode 1203\tAverage Score: 13.88\n",
      "Environment solved in 1103 episodes!\tAverage Score: 13.88\n",
      "Episode 1204\tAverage Score: 13.93\n",
      "Environment solved in 1104 episodes!\tAverage Score: 13.93\n",
      "Episode 1205\tAverage Score: 14.01\n",
      "Environment solved in 1105 episodes!\tAverage Score: 14.01\n",
      "Episode 1206\tAverage Score: 14.11\n",
      "Environment solved in 1106 episodes!\tAverage Score: 14.11\n",
      "Episode 1207\tAverage Score: 14.14\n",
      "Environment solved in 1107 episodes!\tAverage Score: 14.14\n",
      "Episode 1208\tAverage Score: 14.14\n",
      "Environment solved in 1108 episodes!\tAverage Score: 14.14\n",
      "Episode 1209\tAverage Score: 14.17\n",
      "Environment solved in 1109 episodes!\tAverage Score: 14.17\n",
      "Episode 1210\tAverage Score: 14.15\n",
      "Environment solved in 1110 episodes!\tAverage Score: 14.15\n",
      "Episode 1211\tAverage Score: 14.25\n",
      "Environment solved in 1111 episodes!\tAverage Score: 14.25\n",
      "Episode 1212\tAverage Score: 14.34\n",
      "Environment solved in 1112 episodes!\tAverage Score: 14.34\n",
      "Episode 1213\tAverage Score: 14.35\n",
      "Environment solved in 1113 episodes!\tAverage Score: 14.35\n",
      "Episode 1214\tAverage Score: 14.37\n",
      "Environment solved in 1114 episodes!\tAverage Score: 14.37\n",
      "Episode 1215\tAverage Score: 14.38\n",
      "Environment solved in 1115 episodes!\tAverage Score: 14.38\n",
      "Episode 1216\tAverage Score: 14.37\n",
      "Environment solved in 1116 episodes!\tAverage Score: 14.37\n",
      "Episode 1217\tAverage Score: 14.47\n",
      "Environment solved in 1117 episodes!\tAverage Score: 14.47\n",
      "Episode 1218\tAverage Score: 14.61\n",
      "Environment solved in 1118 episodes!\tAverage Score: 14.61\n",
      "Episode 1219\tAverage Score: 14.65\n",
      "Environment solved in 1119 episodes!\tAverage Score: 14.65\n",
      "Episode 1220\tAverage Score: 14.74\n",
      "Environment solved in 1120 episodes!\tAverage Score: 14.74\n",
      "Episode 1221\tAverage Score: 14.72\n",
      "Environment solved in 1121 episodes!\tAverage Score: 14.72\n",
      "Episode 1222\tAverage Score: 14.74\n",
      "Environment solved in 1122 episodes!\tAverage Score: 14.74\n",
      "Episode 1223\tAverage Score: 14.79\n",
      "Environment solved in 1123 episodes!\tAverage Score: 14.79\n",
      "Episode 1224\tAverage Score: 14.82\n",
      "Environment solved in 1124 episodes!\tAverage Score: 14.82\n",
      "Episode 1225\tAverage Score: 14.81\n",
      "Environment solved in 1125 episodes!\tAverage Score: 14.81\n",
      "Episode 1226\tAverage Score: 14.81\n",
      "Environment solved in 1126 episodes!\tAverage Score: 14.81\n",
      "Episode 1227\tAverage Score: 14.83\n",
      "Environment solved in 1127 episodes!\tAverage Score: 14.83\n",
      "Episode 1228\tAverage Score: 14.88\n",
      "Environment solved in 1128 episodes!\tAverage Score: 14.88\n",
      "Episode 1229\tAverage Score: 14.84\n",
      "Environment solved in 1129 episodes!\tAverage Score: 14.84\n",
      "Episode 1230\tAverage Score: 14.78\n",
      "Environment solved in 1130 episodes!\tAverage Score: 14.78\n",
      "Episode 1231\tAverage Score: 14.76\n",
      "Environment solved in 1131 episodes!\tAverage Score: 14.76\n",
      "Episode 1232\tAverage Score: 14.75\n",
      "Environment solved in 1132 episodes!\tAverage Score: 14.75\n",
      "Episode 1233\tAverage Score: 14.76\n",
      "Environment solved in 1133 episodes!\tAverage Score: 14.76\n",
      "Episode 1234\tAverage Score: 14.70\n",
      "Environment solved in 1134 episodes!\tAverage Score: 14.70\n",
      "Episode 1235\tAverage Score: 14.63\n",
      "Environment solved in 1135 episodes!\tAverage Score: 14.63\n",
      "Episode 1236\tAverage Score: 14.52\n",
      "Environment solved in 1136 episodes!\tAverage Score: 14.52\n",
      "Episode 1237\tAverage Score: 14.54\n",
      "Environment solved in 1137 episodes!\tAverage Score: 14.54\n",
      "Episode 1238\tAverage Score: 14.52\n",
      "Environment solved in 1138 episodes!\tAverage Score: 14.52\n",
      "Episode 1239\tAverage Score: 14.46\n",
      "Environment solved in 1139 episodes!\tAverage Score: 14.46\n",
      "Episode 1240\tAverage Score: 14.48\n",
      "Environment solved in 1140 episodes!\tAverage Score: 14.48\n",
      "Episode 1241\tAverage Score: 14.48\n",
      "Environment solved in 1141 episodes!\tAverage Score: 14.48\n",
      "Episode 1242\tAverage Score: 14.46\n",
      "Environment solved in 1142 episodes!\tAverage Score: 14.46\n",
      "Episode 1243\tAverage Score: 14.45\n",
      "Environment solved in 1143 episodes!\tAverage Score: 14.45\n",
      "Episode 1244\tAverage Score: 14.45\n",
      "Environment solved in 1144 episodes!\tAverage Score: 14.45\n",
      "Episode 1245\tAverage Score: 14.41\n",
      "Environment solved in 1145 episodes!\tAverage Score: 14.41\n",
      "Episode 1246\tAverage Score: 14.40\n",
      "Environment solved in 1146 episodes!\tAverage Score: 14.40\n",
      "Episode 1247\tAverage Score: 14.40\n",
      "Environment solved in 1147 episodes!\tAverage Score: 14.40\n",
      "Episode 1248\tAverage Score: 14.35\n",
      "Environment solved in 1148 episodes!\tAverage Score: 14.35\n",
      "Episode 1249\tAverage Score: 14.27\n",
      "Environment solved in 1149 episodes!\tAverage Score: 14.27\n",
      "Episode 1250\tAverage Score: 14.24\n",
      "Environment solved in 1150 episodes!\tAverage Score: 14.24\n",
      "Episode 1251\tAverage Score: 14.16\n",
      "Environment solved in 1151 episodes!\tAverage Score: 14.16\n",
      "Episode 1252\tAverage Score: 14.12\n",
      "Environment solved in 1152 episodes!\tAverage Score: 14.12\n",
      "Episode 1253\tAverage Score: 14.19\n",
      "Environment solved in 1153 episodes!\tAverage Score: 14.19\n",
      "Episode 1254\tAverage Score: 14.26\n",
      "Environment solved in 1154 episodes!\tAverage Score: 14.26\n",
      "Episode 1255\tAverage Score: 14.27\n",
      "Environment solved in 1155 episodes!\tAverage Score: 14.27\n",
      "Episode 1256\tAverage Score: 14.24\n",
      "Environment solved in 1156 episodes!\tAverage Score: 14.24\n",
      "Episode 1257\tAverage Score: 14.25\n",
      "Environment solved in 1157 episodes!\tAverage Score: 14.25\n",
      "Episode 1258\tAverage Score: 14.34\n",
      "Environment solved in 1158 episodes!\tAverage Score: 14.34\n",
      "Episode 1259\tAverage Score: 14.30\n",
      "Environment solved in 1159 episodes!\tAverage Score: 14.30\n",
      "Episode 1260\tAverage Score: 14.26\n",
      "Environment solved in 1160 episodes!\tAverage Score: 14.26\n",
      "Episode 1261\tAverage Score: 14.24\n",
      "Environment solved in 1161 episodes!\tAverage Score: 14.24\n",
      "Episode 1262\tAverage Score: 14.31\n",
      "Environment solved in 1162 episodes!\tAverage Score: 14.31\n",
      "Episode 1263\tAverage Score: 14.42\n",
      "Environment solved in 1163 episodes!\tAverage Score: 14.42\n",
      "Episode 1264\tAverage Score: 14.41\n",
      "Environment solved in 1164 episodes!\tAverage Score: 14.41\n",
      "Episode 1265\tAverage Score: 14.40\n",
      "Environment solved in 1165 episodes!\tAverage Score: 14.40\n",
      "Episode 1266\tAverage Score: 14.40\n",
      "Environment solved in 1166 episodes!\tAverage Score: 14.40\n",
      "Episode 1267\tAverage Score: 14.42\n",
      "Environment solved in 1167 episodes!\tAverage Score: 14.42\n",
      "Episode 1268\tAverage Score: 14.39\n",
      "Environment solved in 1168 episodes!\tAverage Score: 14.39\n",
      "Episode 1269\tAverage Score: 14.35\n",
      "Environment solved in 1169 episodes!\tAverage Score: 14.35\n",
      "Episode 1270\tAverage Score: 14.36\n",
      "Environment solved in 1170 episodes!\tAverage Score: 14.36\n",
      "Episode 1271\tAverage Score: 14.47\n",
      "Environment solved in 1171 episodes!\tAverage Score: 14.47\n",
      "Episode 1272\tAverage Score: 14.47\n",
      "Environment solved in 1172 episodes!\tAverage Score: 14.47\n",
      "Episode 1273\tAverage Score: 14.55\n",
      "Environment solved in 1173 episodes!\tAverage Score: 14.55\n",
      "Episode 1274\tAverage Score: 14.41\n",
      "Environment solved in 1174 episodes!\tAverage Score: 14.41\n",
      "Episode 1275\tAverage Score: 14.37\n",
      "Environment solved in 1175 episodes!\tAverage Score: 14.37\n",
      "Episode 1276\tAverage Score: 14.34\n",
      "Environment solved in 1176 episodes!\tAverage Score: 14.34\n",
      "Episode 1277\tAverage Score: 14.33\n",
      "Environment solved in 1177 episodes!\tAverage Score: 14.33\n",
      "Episode 1278\tAverage Score: 14.28\n",
      "Environment solved in 1178 episodes!\tAverage Score: 14.28\n",
      "Episode 1279\tAverage Score: 14.34\n",
      "Environment solved in 1179 episodes!\tAverage Score: 14.34\n",
      "Episode 1280\tAverage Score: 14.31\n",
      "Environment solved in 1180 episodes!\tAverage Score: 14.31\n",
      "Episode 1281\tAverage Score: 14.28\n",
      "Environment solved in 1181 episodes!\tAverage Score: 14.28\n",
      "Episode 1282\tAverage Score: 14.26\n",
      "Environment solved in 1182 episodes!\tAverage Score: 14.26\n",
      "Episode 1283\tAverage Score: 14.19\n",
      "Environment solved in 1183 episodes!\tAverage Score: 14.19\n",
      "Episode 1284\tAverage Score: 14.18\n",
      "Environment solved in 1184 episodes!\tAverage Score: 14.18\n",
      "Episode 1285\tAverage Score: 14.20\n",
      "Environment solved in 1185 episodes!\tAverage Score: 14.20\n",
      "Episode 1286\tAverage Score: 14.22\n",
      "Environment solved in 1186 episodes!\tAverage Score: 14.22\n",
      "Episode 1287\tAverage Score: 14.26\n",
      "Environment solved in 1187 episodes!\tAverage Score: 14.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1288\tAverage Score: 14.20\n",
      "Environment solved in 1188 episodes!\tAverage Score: 14.20\n",
      "Episode 1289\tAverage Score: 14.18\n",
      "Environment solved in 1189 episodes!\tAverage Score: 14.18\n",
      "Episode 1290\tAverage Score: 14.13\n",
      "Environment solved in 1190 episodes!\tAverage Score: 14.13\n",
      "Episode 1291\tAverage Score: 14.16\n",
      "Environment solved in 1191 episodes!\tAverage Score: 14.16\n",
      "Episode 1292\tAverage Score: 14.10\n",
      "Environment solved in 1192 episodes!\tAverage Score: 14.10\n",
      "Episode 1293\tAverage Score: 14.08\n",
      "Environment solved in 1193 episodes!\tAverage Score: 14.08\n",
      "Episode 1294\tAverage Score: 14.01\n",
      "Environment solved in 1194 episodes!\tAverage Score: 14.01\n",
      "Episode 1295\tAverage Score: 14.05\n",
      "Environment solved in 1195 episodes!\tAverage Score: 14.05\n",
      "Episode 1296\tAverage Score: 14.12\n",
      "Environment solved in 1196 episodes!\tAverage Score: 14.12\n",
      "Episode 1297\tAverage Score: 14.10\n",
      "Environment solved in 1197 episodes!\tAverage Score: 14.10\n",
      "Episode 1298\tAverage Score: 14.06\n",
      "Environment solved in 1198 episodes!\tAverage Score: 14.06\n",
      "Episode 1299\tAverage Score: 14.08\n",
      "Environment solved in 1199 episodes!\tAverage Score: 14.08\n",
      "Episode 1300\tAverage Score: 14.04\n",
      "\n",
      "Environment solved in 1200 episodes!\tAverage Score: 14.04\n",
      "Episode 1301\tAverage Score: 14.08\n",
      "Environment solved in 1201 episodes!\tAverage Score: 14.08\n",
      "Episode 1302\tAverage Score: 13.99\n",
      "Environment solved in 1202 episodes!\tAverage Score: 13.99\n",
      "Episode 1303\tAverage Score: 13.93\n",
      "Environment solved in 1203 episodes!\tAverage Score: 13.93\n",
      "Episode 1304\tAverage Score: 13.90\n",
      "Environment solved in 1204 episodes!\tAverage Score: 13.90\n",
      "Episode 1305\tAverage Score: 13.88\n",
      "Environment solved in 1205 episodes!\tAverage Score: 13.88\n",
      "Episode 1306\tAverage Score: 13.86\n",
      "Environment solved in 1206 episodes!\tAverage Score: 13.86\n",
      "Episode 1307\tAverage Score: 13.81\n",
      "Environment solved in 1207 episodes!\tAverage Score: 13.81\n",
      "Episode 1308\tAverage Score: 13.83\n",
      "Environment solved in 1208 episodes!\tAverage Score: 13.83\n",
      "Episode 1309\tAverage Score: 13.74\n",
      "Environment solved in 1209 episodes!\tAverage Score: 13.74\n",
      "Episode 1310\tAverage Score: 13.67\n",
      "Environment solved in 1210 episodes!\tAverage Score: 13.67\n",
      "Episode 1311\tAverage Score: 13.57\n",
      "Environment solved in 1211 episodes!\tAverage Score: 13.57\n",
      "Episode 1312\tAverage Score: 13.46\n",
      "Environment solved in 1212 episodes!\tAverage Score: 13.46\n",
      "Episode 1313\tAverage Score: 13.42\n",
      "Environment solved in 1213 episodes!\tAverage Score: 13.42\n",
      "Episode 1314\tAverage Score: 13.43\n",
      "Environment solved in 1214 episodes!\tAverage Score: 13.43\n",
      "Episode 1315\tAverage Score: 13.36\n",
      "Environment solved in 1215 episodes!\tAverage Score: 13.36\n",
      "Episode 1316\tAverage Score: 13.32\n",
      "Environment solved in 1216 episodes!\tAverage Score: 13.32\n",
      "Episode 1317\tAverage Score: 13.21\n",
      "Environment solved in 1217 episodes!\tAverage Score: 13.21\n",
      "Episode 1318\tAverage Score: 13.06\n",
      "Environment solved in 1218 episodes!\tAverage Score: 13.06\n",
      "Episode 1400\tAverage Score: 5.963\n",
      "Episode 1500\tAverage Score: 4.37\n",
      "Episode 1600\tAverage Score: 10.91\n",
      "Episode 1700\tAverage Score: 12.12\n",
      "Episode 1800\tAverage Score: 5.032\n",
      "Episode 1900\tAverage Score: 8.39\n",
      "Episode 2000\tAverage Score: 8.26\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rl.value.agent import DDQN_UER_Agent\n",
    "from rl.value.model import DuelingQNetwork\n",
    "from rl.utils.policy import EpsilonGreedy\n",
    "\n",
    "params = {\n",
    "    'name': 'DDDQN_UER',\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'max_t': 5000,               # maximum length of an episode\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 1.0,            # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,             # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 0.9975          # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'agent_params': {\n",
    "        'alpha': 0.4,                # alpha power value for the prioritized replay buffer sampling\n",
    "        'gamma': 0.99,               # discount factor\n",
    "        'tau': 0.001,                # mixing rate soft-update of target parameters\n",
    "        'lr': 0.00005,               # learning rate\n",
    "        'update_every': 4,           # update every n-th step\n",
    "        'network_type': DuelingQNetwork,    # network architecture\n",
    "        'network_params': {\n",
    "            'state_size': state_size,    # size of the state space\n",
    "            'action_size': action_size,  # size of the action space\n",
    "            'seed': 6453,                # seed of the network architecture\n",
    "            'hidden_layers': [128, 64],  # hidden layer neurons\n",
    "            'dropout': 0.01\n",
    "        },\n",
    "        'experience_params': {\n",
    "            'seed': 184,                 # seed for the experience replay buffer\n",
    "            'buffer_size': 1000,         # size of the replay buffer\n",
    "            'batch_size': 64             # batch size sampled from the replay buffer\n",
    "        },\n",
    "    },\n",
    "    'maxlen': 100,               # sliding window size of recent scores\n",
    "    'beta_start': 0.4,           # starting value for the beta value of prioritized replay sampling\n",
    "    'brain_name': brain_name     # the brain name of the unity environment\n",
    "}\n",
    "\n",
    "agent = DDQN_UER_Agent(params=params.get('agent_params', None))\n",
    "scores = train(agent=agent, env=env, params=params)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'DDDQN_UER': scores})\n",
    "df.to_csv('results/DDDQN_UER_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 DQN Agent with Prioritized Experience Replay\n",
    "\n",
    "This experiment configures a Double Q-Network agent with prioritized experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:rl.agent:Parameter: {'alpha': 0.4, 'gamma': 0.99, 'tau': 0.001, 'lr': 5e-05, 'update_every': 4, 'network_type': <class 'rl.model.QNetwork'>, 'network_params': {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}, 'experience_params': {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}}\n",
      "DEBUG:rl.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}\n",
      "DEBUG:rl.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}\n",
      "DEBUG:rl.buffer:Parameter: {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}\n",
      "DEBUG:rl.policy:Parameter: {'eps_start': 1.0, 'eps_end': 0.01, 'eps_decay': 0.9975}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.14\n",
      "Episode 200\tAverage Score: 0.40\n",
      "Episode 300\tAverage Score: 1.32\n",
      "Episode 400\tAverage Score: 2.19\n",
      "Episode 500\tAverage Score: 4.43\n",
      "Episode 600\tAverage Score: 5.58\n",
      "Episode 700\tAverage Score: 6.24\n",
      "Episode 800\tAverage Score: 8.65\n",
      "Episode 900\tAverage Score: 9.61\n",
      "Episode 1000\tAverage Score: 7.45\n",
      "Episode 1100\tAverage Score: 9.73\n",
      "Episode 1200\tAverage Score: 10.89\n",
      "Episode 1284\tAverage Score: 13.05\n",
      "Environment solved in 1184 episodes!\tAverage Score: 13.05\n",
      "Episode 1285\tAverage Score: 13.11\n",
      "Environment solved in 1185 episodes!\tAverage Score: 13.11\n",
      "Episode 1286\tAverage Score: 13.21\n",
      "Environment solved in 1186 episodes!\tAverage Score: 13.21\n",
      "Episode 1287\tAverage Score: 13.13\n",
      "Environment solved in 1187 episodes!\tAverage Score: 13.13\n",
      "Episode 1288\tAverage Score: 13.16\n",
      "Environment solved in 1188 episodes!\tAverage Score: 13.16\n",
      "Episode 1289\tAverage Score: 13.21\n",
      "Environment solved in 1189 episodes!\tAverage Score: 13.21\n",
      "Episode 1290\tAverage Score: 13.15\n",
      "Environment solved in 1190 episodes!\tAverage Score: 13.15\n",
      "Episode 1291\tAverage Score: 13.15\n",
      "Environment solved in 1191 episodes!\tAverage Score: 13.15\n",
      "Episode 1292\tAverage Score: 13.22\n",
      "Environment solved in 1192 episodes!\tAverage Score: 13.22\n",
      "Episode 1293\tAverage Score: 13.30\n",
      "Environment solved in 1193 episodes!\tAverage Score: 13.30\n",
      "Episode 1294\tAverage Score: 13.44\n",
      "Environment solved in 1194 episodes!\tAverage Score: 13.44\n",
      "Episode 1295\tAverage Score: 13.38\n",
      "Environment solved in 1195 episodes!\tAverage Score: 13.38\n",
      "Episode 1296\tAverage Score: 13.34\n",
      "Environment solved in 1196 episodes!\tAverage Score: 13.34\n",
      "Episode 1297\tAverage Score: 13.27\n",
      "Environment solved in 1197 episodes!\tAverage Score: 13.27\n",
      "Episode 1298\tAverage Score: 13.27\n",
      "Environment solved in 1198 episodes!\tAverage Score: 13.27\n",
      "Episode 1299\tAverage Score: 13.19\n",
      "Environment solved in 1199 episodes!\tAverage Score: 13.19\n",
      "Episode 1300\tAverage Score: 13.13\n",
      "\n",
      "Environment solved in 1200 episodes!\tAverage Score: 13.13\n",
      "Episode 1301\tAverage Score: 13.05\n",
      "Environment solved in 1201 episodes!\tAverage Score: 13.05\n",
      "Episode 1302\tAverage Score: 13.00\n",
      "Environment solved in 1202 episodes!\tAverage Score: 13.00\n",
      "Episode 1400\tAverage Score: 10.54\n",
      "Episode 1500\tAverage Score: 11.39\n",
      "Episode 1559\tAverage Score: 13.07\n",
      "Environment solved in 1459 episodes!\tAverage Score: 13.07\n",
      "Episode 1560\tAverage Score: 13.24\n",
      "Environment solved in 1460 episodes!\tAverage Score: 13.24\n",
      "Episode 1561\tAverage Score: 13.28\n",
      "Environment solved in 1461 episodes!\tAverage Score: 13.28\n",
      "Episode 1562\tAverage Score: 13.25\n",
      "Environment solved in 1462 episodes!\tAverage Score: 13.25\n",
      "Episode 1563\tAverage Score: 13.24\n",
      "Environment solved in 1463 episodes!\tAverage Score: 13.24\n",
      "Episode 1564\tAverage Score: 13.26\n",
      "Environment solved in 1464 episodes!\tAverage Score: 13.26\n",
      "Episode 1565\tAverage Score: 13.34\n",
      "Environment solved in 1465 episodes!\tAverage Score: 13.34\n",
      "Episode 1566\tAverage Score: 13.34\n",
      "Environment solved in 1466 episodes!\tAverage Score: 13.34\n",
      "Episode 1567\tAverage Score: 13.49\n",
      "Environment solved in 1467 episodes!\tAverage Score: 13.49\n",
      "Episode 1568\tAverage Score: 13.64\n",
      "Environment solved in 1468 episodes!\tAverage Score: 13.64\n",
      "Episode 1569\tAverage Score: 13.60\n",
      "Environment solved in 1469 episodes!\tAverage Score: 13.60\n",
      "Episode 1570\tAverage Score: 13.57\n",
      "Environment solved in 1470 episodes!\tAverage Score: 13.57\n",
      "Episode 1571\tAverage Score: 13.50\n",
      "Environment solved in 1471 episodes!\tAverage Score: 13.50\n",
      "Episode 1572\tAverage Score: 13.60\n",
      "Environment solved in 1472 episodes!\tAverage Score: 13.60\n",
      "Episode 1573\tAverage Score: 13.83\n",
      "Environment solved in 1473 episodes!\tAverage Score: 13.83\n",
      "Episode 1574\tAverage Score: 13.71\n",
      "Environment solved in 1474 episodes!\tAverage Score: 13.71\n",
      "Episode 1575\tAverage Score: 13.86\n",
      "Environment solved in 1475 episodes!\tAverage Score: 13.86\n",
      "Episode 1576\tAverage Score: 13.83\n",
      "Environment solved in 1476 episodes!\tAverage Score: 13.83\n",
      "Episode 1577\tAverage Score: 13.80\n",
      "Environment solved in 1477 episodes!\tAverage Score: 13.80\n",
      "Episode 1578\tAverage Score: 13.83\n",
      "Environment solved in 1478 episodes!\tAverage Score: 13.83\n",
      "Episode 1579\tAverage Score: 13.76\n",
      "Environment solved in 1479 episodes!\tAverage Score: 13.76\n",
      "Episode 1580\tAverage Score: 13.68\n",
      "Environment solved in 1480 episodes!\tAverage Score: 13.68\n",
      "Episode 1581\tAverage Score: 13.66\n",
      "Environment solved in 1481 episodes!\tAverage Score: 13.66\n",
      "Episode 1582\tAverage Score: 13.64\n",
      "Environment solved in 1482 episodes!\tAverage Score: 13.64\n",
      "Episode 1583\tAverage Score: 13.62\n",
      "Environment solved in 1483 episodes!\tAverage Score: 13.62\n",
      "Episode 1584\tAverage Score: 13.74\n",
      "Environment solved in 1484 episodes!\tAverage Score: 13.74\n",
      "Episode 1585\tAverage Score: 13.79\n",
      "Environment solved in 1485 episodes!\tAverage Score: 13.79\n",
      "Episode 1586\tAverage Score: 13.82\n",
      "Environment solved in 1486 episodes!\tAverage Score: 13.82\n",
      "Episode 1587\tAverage Score: 13.75\n",
      "Environment solved in 1487 episodes!\tAverage Score: 13.75\n",
      "Episode 1588\tAverage Score: 13.70\n",
      "Environment solved in 1488 episodes!\tAverage Score: 13.70\n",
      "Episode 1589\tAverage Score: 13.61\n",
      "Environment solved in 1489 episodes!\tAverage Score: 13.61\n",
      "Episode 1590\tAverage Score: 13.56\n",
      "Environment solved in 1490 episodes!\tAverage Score: 13.56\n",
      "Episode 1591\tAverage Score: 13.63\n",
      "Environment solved in 1491 episodes!\tAverage Score: 13.63\n",
      "Episode 1592\tAverage Score: 13.74\n",
      "Environment solved in 1492 episodes!\tAverage Score: 13.74\n",
      "Episode 1593\tAverage Score: 13.70\n",
      "Environment solved in 1493 episodes!\tAverage Score: 13.70\n",
      "Episode 1594\tAverage Score: 13.72\n",
      "Environment solved in 1494 episodes!\tAverage Score: 13.72\n",
      "Episode 1595\tAverage Score: 13.76\n",
      "Environment solved in 1495 episodes!\tAverage Score: 13.76\n",
      "Episode 1596\tAverage Score: 13.72\n",
      "Environment solved in 1496 episodes!\tAverage Score: 13.72\n",
      "Episode 1597\tAverage Score: 13.73\n",
      "Environment solved in 1497 episodes!\tAverage Score: 13.73\n",
      "Episode 1598\tAverage Score: 13.68\n",
      "Environment solved in 1498 episodes!\tAverage Score: 13.68\n",
      "Episode 1599\tAverage Score: 13.70\n",
      "Environment solved in 1499 episodes!\tAverage Score: 13.70\n",
      "Episode 1600\tAverage Score: 13.75\n",
      "\n",
      "Environment solved in 1500 episodes!\tAverage Score: 13.75\n",
      "Episode 1601\tAverage Score: 13.86\n",
      "Environment solved in 1501 episodes!\tAverage Score: 13.86\n",
      "Episode 1602\tAverage Score: 13.90\n",
      "Environment solved in 1502 episodes!\tAverage Score: 13.90\n",
      "Episode 1603\tAverage Score: 13.84\n",
      "Environment solved in 1503 episodes!\tAverage Score: 13.84\n",
      "Episode 1604\tAverage Score: 13.89\n",
      "Environment solved in 1504 episodes!\tAverage Score: 13.89\n",
      "Episode 1605\tAverage Score: 13.92\n",
      "Environment solved in 1505 episodes!\tAverage Score: 13.92\n",
      "Episode 1606\tAverage Score: 13.93\n",
      "Environment solved in 1506 episodes!\tAverage Score: 13.93\n",
      "Episode 1607\tAverage Score: 13.94\n",
      "Environment solved in 1507 episodes!\tAverage Score: 13.94\n",
      "Episode 1608\tAverage Score: 13.97\n",
      "Environment solved in 1508 episodes!\tAverage Score: 13.97\n",
      "Episode 1609\tAverage Score: 13.94\n",
      "Environment solved in 1509 episodes!\tAverage Score: 13.94\n",
      "Episode 1610\tAverage Score: 13.93\n",
      "Environment solved in 1510 episodes!\tAverage Score: 13.93\n",
      "Episode 1611\tAverage Score: 13.96\n",
      "Environment solved in 1511 episodes!\tAverage Score: 13.96\n",
      "Episode 1612\tAverage Score: 14.04\n",
      "Environment solved in 1512 episodes!\tAverage Score: 14.04\n",
      "Episode 1613\tAverage Score: 14.01\n",
      "Environment solved in 1513 episodes!\tAverage Score: 14.01\n",
      "Episode 1614\tAverage Score: 13.97\n",
      "Environment solved in 1514 episodes!\tAverage Score: 13.97\n",
      "Episode 1615\tAverage Score: 13.92\n",
      "Environment solved in 1515 episodes!\tAverage Score: 13.92\n",
      "Episode 1616\tAverage Score: 13.81\n",
      "Environment solved in 1516 episodes!\tAverage Score: 13.81\n",
      "Episode 1617\tAverage Score: 13.79\n",
      "Environment solved in 1517 episodes!\tAverage Score: 13.79\n",
      "Episode 1618\tAverage Score: 13.79\n",
      "Environment solved in 1518 episodes!\tAverage Score: 13.79\n",
      "Episode 1619\tAverage Score: 13.79\n",
      "Environment solved in 1519 episodes!\tAverage Score: 13.79\n",
      "Episode 1620\tAverage Score: 13.74\n",
      "Environment solved in 1520 episodes!\tAverage Score: 13.74\n",
      "Episode 1621\tAverage Score: 13.88\n",
      "Environment solved in 1521 episodes!\tAverage Score: 13.88\n",
      "Episode 1622\tAverage Score: 13.96\n",
      "Environment solved in 1522 episodes!\tAverage Score: 13.96\n",
      "Episode 1623\tAverage Score: 14.08\n",
      "Environment solved in 1523 episodes!\tAverage Score: 14.08\n",
      "Episode 1624\tAverage Score: 14.12\n",
      "Environment solved in 1524 episodes!\tAverage Score: 14.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1625\tAverage Score: 14.13\n",
      "Environment solved in 1525 episodes!\tAverage Score: 14.13\n",
      "Episode 1626\tAverage Score: 14.16\n",
      "Environment solved in 1526 episodes!\tAverage Score: 14.16\n",
      "Episode 1627\tAverage Score: 14.12\n",
      "Environment solved in 1527 episodes!\tAverage Score: 14.12\n",
      "Episode 1628\tAverage Score: 14.09\n",
      "Environment solved in 1528 episodes!\tAverage Score: 14.09\n",
      "Episode 1629\tAverage Score: 13.98\n",
      "Environment solved in 1529 episodes!\tAverage Score: 13.98\n",
      "Episode 1630\tAverage Score: 13.88\n",
      "Environment solved in 1530 episodes!\tAverage Score: 13.88\n",
      "Episode 1631\tAverage Score: 13.87\n",
      "Environment solved in 1531 episodes!\tAverage Score: 13.87\n",
      "Episode 1632\tAverage Score: 13.83\n",
      "Environment solved in 1532 episodes!\tAverage Score: 13.83\n",
      "Episode 1633\tAverage Score: 13.85\n",
      "Environment solved in 1533 episodes!\tAverage Score: 13.85\n",
      "Episode 1634\tAverage Score: 13.89\n",
      "Environment solved in 1534 episodes!\tAverage Score: 13.89\n",
      "Episode 1635\tAverage Score: 13.91\n",
      "Environment solved in 1535 episodes!\tAverage Score: 13.91\n",
      "Episode 1636\tAverage Score: 13.77\n",
      "Environment solved in 1536 episodes!\tAverage Score: 13.77\n",
      "Episode 1637\tAverage Score: 13.61\n",
      "Environment solved in 1537 episodes!\tAverage Score: 13.61\n",
      "Episode 1638\tAverage Score: 13.59\n",
      "Environment solved in 1538 episodes!\tAverage Score: 13.59\n",
      "Episode 1639\tAverage Score: 13.71\n",
      "Environment solved in 1539 episodes!\tAverage Score: 13.71\n",
      "Episode 1640\tAverage Score: 13.66\n",
      "Environment solved in 1540 episodes!\tAverage Score: 13.66\n",
      "Episode 1641\tAverage Score: 13.56\n",
      "Environment solved in 1541 episodes!\tAverage Score: 13.56\n",
      "Episode 1642\tAverage Score: 13.51\n",
      "Environment solved in 1542 episodes!\tAverage Score: 13.51\n",
      "Episode 1643\tAverage Score: 13.39\n",
      "Environment solved in 1543 episodes!\tAverage Score: 13.39\n",
      "Episode 1644\tAverage Score: 13.28\n",
      "Environment solved in 1544 episodes!\tAverage Score: 13.28\n",
      "Episode 1645\tAverage Score: 13.13\n",
      "Environment solved in 1545 episodes!\tAverage Score: 13.13\n",
      "Episode 1646\tAverage Score: 13.05\n",
      "Environment solved in 1546 episodes!\tAverage Score: 13.05\n",
      "Episode 1647\tAverage Score: 13.01\n",
      "Environment solved in 1547 episodes!\tAverage Score: 13.01\n",
      "Episode 1700\tAverage Score: 11.20\n",
      "Episode 1800\tAverage Score: 10.48\n",
      "Episode 1900\tAverage Score: 12.66\n",
      "Episode 1906\tAverage Score: 13.14\n",
      "Environment solved in 1806 episodes!\tAverage Score: 13.14\n",
      "Episode 1907\tAverage Score: 13.17\n",
      "Environment solved in 1807 episodes!\tAverage Score: 13.17\n",
      "Episode 1908\tAverage Score: 13.11\n",
      "Environment solved in 1808 episodes!\tAverage Score: 13.11\n",
      "Episode 1909\tAverage Score: 13.10\n",
      "Environment solved in 1809 episodes!\tAverage Score: 13.10\n",
      "Episode 1910\tAverage Score: 13.05\n",
      "Environment solved in 1810 episodes!\tAverage Score: 13.05\n",
      "Episode 1911\tAverage Score: 13.08\n",
      "Environment solved in 1811 episodes!\tAverage Score: 13.08\n",
      "Episode 1912\tAverage Score: 13.09\n",
      "Environment solved in 1812 episodes!\tAverage Score: 13.09\n",
      "Episode 1913\tAverage Score: 13.14\n",
      "Environment solved in 1813 episodes!\tAverage Score: 13.14\n",
      "Episode 1914\tAverage Score: 13.14\n",
      "Environment solved in 1814 episodes!\tAverage Score: 13.14\n",
      "Episode 1915\tAverage Score: 13.11\n",
      "Environment solved in 1815 episodes!\tAverage Score: 13.11\n",
      "Episode 1922\tAverage Score: 13.02\n",
      "Environment solved in 1822 episodes!\tAverage Score: 13.02\n",
      "Episode 1944\tAverage Score: 13.09\n",
      "Environment solved in 1844 episodes!\tAverage Score: 13.09\n",
      "Episode 1945\tAverage Score: 13.10\n",
      "Environment solved in 1845 episodes!\tAverage Score: 13.10\n",
      "Episode 1946\tAverage Score: 13.12\n",
      "Environment solved in 1846 episodes!\tAverage Score: 13.12\n",
      "Episode 1947\tAverage Score: 13.18\n",
      "Environment solved in 1847 episodes!\tAverage Score: 13.18\n",
      "Episode 1948\tAverage Score: 13.36\n",
      "Environment solved in 1848 episodes!\tAverage Score: 13.36\n",
      "Episode 1949\tAverage Score: 13.32\n",
      "Environment solved in 1849 episodes!\tAverage Score: 13.32\n",
      "Episode 1950\tAverage Score: 13.39\n",
      "Environment solved in 1850 episodes!\tAverage Score: 13.39\n",
      "Episode 1951\tAverage Score: 13.51\n",
      "Environment solved in 1851 episodes!\tAverage Score: 13.51\n",
      "Episode 1952\tAverage Score: 13.50\n",
      "Environment solved in 1852 episodes!\tAverage Score: 13.50\n",
      "Episode 1953\tAverage Score: 13.51\n",
      "Environment solved in 1853 episodes!\tAverage Score: 13.51\n",
      "Episode 1954\tAverage Score: 13.49\n",
      "Environment solved in 1854 episodes!\tAverage Score: 13.49\n",
      "Episode 1955\tAverage Score: 13.45\n",
      "Environment solved in 1855 episodes!\tAverage Score: 13.45\n",
      "Episode 1956\tAverage Score: 13.49\n",
      "Environment solved in 1856 episodes!\tAverage Score: 13.49\n",
      "Episode 1957\tAverage Score: 13.65\n",
      "Environment solved in 1857 episodes!\tAverage Score: 13.65\n",
      "Episode 1958\tAverage Score: 13.58\n",
      "Environment solved in 1858 episodes!\tAverage Score: 13.58\n",
      "Episode 1959\tAverage Score: 13.58\n",
      "Environment solved in 1859 episodes!\tAverage Score: 13.58\n",
      "Episode 1960\tAverage Score: 13.50\n",
      "Environment solved in 1860 episodes!\tAverage Score: 13.50\n",
      "Episode 1961\tAverage Score: 13.67\n",
      "Environment solved in 1861 episodes!\tAverage Score: 13.67\n",
      "Episode 1962\tAverage Score: 13.69\n",
      "Environment solved in 1862 episodes!\tAverage Score: 13.69\n",
      "Episode 1963\tAverage Score: 13.63\n",
      "Environment solved in 1863 episodes!\tAverage Score: 13.63\n",
      "Episode 1964\tAverage Score: 13.61\n",
      "Environment solved in 1864 episodes!\tAverage Score: 13.61\n",
      "Episode 1965\tAverage Score: 13.61\n",
      "Environment solved in 1865 episodes!\tAverage Score: 13.61\n",
      "Episode 1966\tAverage Score: 13.66\n",
      "Environment solved in 1866 episodes!\tAverage Score: 13.66\n",
      "Episode 1967\tAverage Score: 13.67\n",
      "Environment solved in 1867 episodes!\tAverage Score: 13.67\n",
      "Episode 1968\tAverage Score: 13.61\n",
      "Environment solved in 1868 episodes!\tAverage Score: 13.61\n",
      "Episode 1969\tAverage Score: 13.57\n",
      "Environment solved in 1869 episodes!\tAverage Score: 13.57\n",
      "Episode 1970\tAverage Score: 13.51\n",
      "Environment solved in 1870 episodes!\tAverage Score: 13.51\n",
      "Episode 1971\tAverage Score: 13.54\n",
      "Environment solved in 1871 episodes!\tAverage Score: 13.54\n",
      "Episode 1972\tAverage Score: 13.41\n",
      "Environment solved in 1872 episodes!\tAverage Score: 13.41\n",
      "Episode 1973\tAverage Score: 13.29\n",
      "Environment solved in 1873 episodes!\tAverage Score: 13.29\n",
      "Episode 1974\tAverage Score: 13.15\n",
      "Environment solved in 1874 episodes!\tAverage Score: 13.15\n",
      "Episode 1975\tAverage Score: 13.12\n",
      "Environment solved in 1875 episodes!\tAverage Score: 13.12\n",
      "Episode 1976\tAverage Score: 13.13\n",
      "Environment solved in 1876 episodes!\tAverage Score: 13.13\n",
      "Episode 1977\tAverage Score: 13.10\n",
      "Environment solved in 1877 episodes!\tAverage Score: 13.10\n",
      "Episode 1978\tAverage Score: 13.09\n",
      "Environment solved in 1878 episodes!\tAverage Score: 13.09\n",
      "Episode 1979\tAverage Score: 13.04\n",
      "Environment solved in 1879 episodes!\tAverage Score: 13.04\n",
      "Episode 1981\tAverage Score: 13.00\n",
      "Environment solved in 1881 episodes!\tAverage Score: 13.00\n",
      "Episode 2000\tAverage Score: 11.92\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rl.value.agent import DDQN_PER_Agent\n",
    "from rl.value.model import QNetwork\n",
    "from rl.utils.policy import EpsilonGreedy\n",
    "\n",
    "params = {\n",
    "    'name': 'DDQN_PER',\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'max_t': 5000,               # maximum length of an episode\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 1.0,            # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,             # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 0.9975          # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'agent_params': {\n",
    "        'alpha': 0.4,                # alpha power value for the prioritized replay buffer sampling\n",
    "        'gamma': 0.99,               # discount factor\n",
    "        'tau': 0.001,                # mixing rate soft-update of target parameters\n",
    "        'lr': 0.00005,               # learning rate\n",
    "        'update_every': 4,           # update every n-th step\n",
    "        'network_type': QNetwork,    # network architecture\n",
    "        'network_params': {\n",
    "            'state_size': state_size,    # size of the state space\n",
    "            'action_size': action_size,  # size of the action space\n",
    "            'seed': 6453,                # seed of the network architecture\n",
    "            'hidden_layers': [128, 64],  # hidden layer neurons\n",
    "            'dropout': 0.3\n",
    "        },\n",
    "        'experience_params': {\n",
    "            'seed': 184,                 # seed for the experience replay buffer\n",
    "            'buffer_size': 1000,         # size of the replay buffer\n",
    "            'batch_size': 64             # batch size sampled from the replay buffer\n",
    "        },\n",
    "    },\n",
    "    'maxlen': 100,               # sliding window size of recent scores\n",
    "    'beta_start': 0.4,           # starting value for the beta value of prioritized replay sampling\n",
    "    'brain_name': brain_name     # the brain name of the unity environment\n",
    "}\n",
    "\n",
    "agent = DDQN_PER_Agent(params=params.get('agent_params', None))\n",
    "scores = train(agent=agent, env=env, params=params)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'DDQN_PER': scores})\n",
    "df.to_csv('results/DDQN_PER_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 DDQN Agent with Prioritized Experience Replay\n",
    "\n",
    "This experiment configures a Dueling Double Q-Network agent with prioritized experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:rl.agent:Parameter: {'alpha': 0.4, 'gamma': 0.99, 'tau': 0.001, 'lr': 5e-05, 'update_every': 4, 'network_type': <class 'rl.model.DuelingQNetwork'>, 'network_params': {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}, 'experience_params': {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}}\n",
      "DEBUG:rl.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}\n",
      "DEBUG:rl.model:Parameter: {'state_size': 37, 'action_size': 4, 'seed': 6453, 'hidden_layers': [128, 64], 'dropout': 0.3}\n",
      "DEBUG:rl.buffer:Parameter: {'seed': 184, 'buffer_size': 1000, 'batch_size': 64}\n",
      "DEBUG:rl.policy:Parameter: {'eps_start': 1.0, 'eps_end': 0.01, 'eps_decay': 0.9975}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -0.08\n",
      "Episode 200\tAverage Score: 0.171\n",
      "Episode 300\tAverage Score: 0.72\n",
      "Episode 400\tAverage Score: 1.06\n",
      "Episode 500\tAverage Score: 0.82\n",
      "Episode 600\tAverage Score: 1.89\n",
      "Episode 700\tAverage Score: 2.62\n",
      "Episode 800\tAverage Score: 2.75\n",
      "Episode 900\tAverage Score: 2.66\n",
      "Episode 1000\tAverage Score: 3.31\n",
      "Episode 1100\tAverage Score: 3.09\n",
      "Episode 1200\tAverage Score: 3.70\n",
      "Episode 1300\tAverage Score: 4.54\n",
      "Episode 1400\tAverage Score: 3.64\n",
      "Episode 1500\tAverage Score: 3.29\n",
      "Episode 1600\tAverage Score: 4.00\n",
      "Episode 1700\tAverage Score: 3.24\n",
      "Episode 1800\tAverage Score: 3.78\n",
      "Episode 1900\tAverage Score: 4.69\n",
      "Episode 2000\tAverage Score: 3.84\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rl.value.agent import DDQN_PER_Agent\n",
    "from rl.value.model import DuelingQNetwork\n",
    "from rl.utils.policy import EpsilonGreedy\n",
    "\n",
    "params = {\n",
    "    'name': 'DDDQN_PER',\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'max_t': 5000,               # maximum length of an episode\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 1.0,            # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,             # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 0.9975          # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'agent_params': {\n",
    "        'alpha': 0.4,                # alpha power value for the prioritized replay buffer sampling\n",
    "        'gamma': 0.99,               # discount factor\n",
    "        'tau': 0.001,                # mixing rate soft-update of target parameters\n",
    "        'lr': 0.00005,               # learning rate\n",
    "        'update_every': 4,           # update every n-th step\n",
    "        'network_type': DuelingQNetwork,    # network architecture\n",
    "        'network_params': {\n",
    "            'state_size': state_size,    # size of the state space\n",
    "            'action_size': action_size,  # size of the action space\n",
    "            'seed': 6453,                # seed of the network architecture\n",
    "            'hidden_layers': [128, 64],  # hidden layer neurons\n",
    "            'dropout': 0.3\n",
    "        },\n",
    "        'experience_params': {\n",
    "            'seed': 184,                 # seed for the experience replay buffer\n",
    "            'buffer_size': 1000,         # size of the replay buffer\n",
    "            'batch_size': 64             # batch size sampled from the replay buffer\n",
    "        },\n",
    "    },\n",
    "    'maxlen': 100,               # sliding window size of recent scores\n",
    "    'beta_start': 0.4,           # starting value for the beta value of prioritized replay sampling\n",
    "    'brain_name': brain_name     # the brain name of the unity environment\n",
    "}\n",
    "\n",
    "agent = DDQN_PER_Agent(params=params.get('agent_params', None))\n",
    "scores = train(agent=agent, env=env, params=params)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), 'DDDQN_PER': scores})\n",
    "df.to_csv('results/DDDQN_PER_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Plot Summary\n",
    "\n",
    "We let all experiments run for an equal number of episodes. Here we summarize our findings into a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x128fd0a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#df = [pd.read_csv(f, index_col='episode') for f in glob.glob('results/*scores.csv')]\n",
    "df = pd.concat([pd.read_csv(f) for f in glob.glob('results/*_scores.csv')], axis=1)\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "dfRolling = df.rolling(100).mean()\n",
    "dfRolling.plot(x='episode', y=['DDDQN_UER', 'DDQN_UER', 'DDQN_PER', 'DDDQN_PER'], kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I admit each of the experiments were not tuned, so I don't know whether any of those reflect on the true performance and robustness of a solution. Yet, the Dueling Double Deep Q-Learning agent with uniform experience replay was the fastest learner and achieved the highest scores. It did deteriorate in subsequent episodes though, which I did not inspect further. As for the prioritized experience replay, it did not outperform the simpler uniform approach, but again this could be due to the $\\alpha$ and $\\beta$ schedules which were not tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policy-based Methods and Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "DEBUG:rl.policy.agent:Parameter: {'name': 'CEM', 'environment': <unityagents.environment.UnityEnvironment object at 0x105ca9400>, 'brain_name': 'BananaBrain', 'episodes': 2000, 'max_t': 5000, 'policy': <class 'rl.utils.policy.EpsilonGreedy'>, 'policy_params': {'eps_start': 0.01, 'eps_end': 0.01, 'eps_decay': 1.0}, 'agent_params': {'gamma': 0.99, 'network_params': {'state_size': 37, 'action_size': 4, 'hidden_layers': [128, 64]}}, 'cross_entropy_params': {'pop_size': 50, 'elite_frac': 0.2, 'sigma': 0.5}, 'maxlen': 100}\n",
      "DEBUG:rl.utils.policy:Parameter: {'eps_start': 0.01, 'eps_end': 0.01, 'eps_decay': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.88\n",
      "Episode 200\tAverage Score: 3.05\n",
      "Episode 300\tAverage Score: 3.39\n",
      "Episode 400\tAverage Score: 3.36\n",
      "Episode 500\tAverage Score: 3.27\n",
      "Episode 600\tAverage Score: 3.42\n",
      "Episode 700\tAverage Score: 3.65\n",
      "Episode 800\tAverage Score: 3.62\n",
      "Episode 900\tAverage Score: 3.76\n",
      "Episode 937\tAverage Score: 3.62"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a928106f5558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/policy/cem.py\u001b[0m in \u001b[0;36mcem\u001b[0;34m(agent, params)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mweights_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbest_weight\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights_pop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0melite_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_elite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/policy/cem.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mweights_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbest_weight\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights_pop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0melite_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_elite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/githubs/deep-reinforcement-learning-navigation/rl/policy/agent.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, weights, policy)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# send the action to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# get the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rl.policy.agent import Agent\n",
    "from rl.policy.cem import cem\n",
    "from rl.utils.policy import EpsilonGreedy\n",
    "\n",
    "params = {\n",
    "    'name': 'CEM',               # Name of the experiment\n",
    "    'environment': env,          # The environment object\n",
    "    'brain_name': brain_name,    # The brain name for the unity environment\n",
    "    'episodes': 2000,            # number of episodes\n",
    "    'max_t': 5000,               # maximum length of an episode\n",
    "    'policy': EpsilonGreedy,\n",
    "    'policy_params': {\n",
    "        'eps_start': 0.01,       # starting value for the epsilon-greedy action selection\n",
    "        'eps_end': 0.01,         # end value for the epsilon-greedy action selection\n",
    "        'eps_decay': 1.          # decay rate for the epsilon-greedy action selection\n",
    "    },\n",
    "    'agent_params': {\n",
    "        'gamma': 0.99,               # discount factor\n",
    "        'network_params': {\n",
    "            'state_size': state_size,    # size of the state space\n",
    "            'action_size': action_size,  # size of the action space\n",
    "            'hidden_layers': [128, 64],  # hidden layer neurons\n",
    "        },\n",
    "    },\n",
    "    'cross_entropy_params': {\n",
    "        'pop_size': 50,\n",
    "        'elite_frac': 0.2,\n",
    "        'sigma': 0.5\n",
    "    },\n",
    "    'maxlen': 100,               # sliding window size of recent scores\n",
    "    'brain_name': brain_name     # the brain name of the unity environment\n",
    "}\n",
    "\n",
    "agent = Agent(params=params)\n",
    "scores = cem(agent=agent, params=params)\n",
    "\n",
    "df = pd.DataFrame(data={'episode': np.arange(len(scores)), params['name']: scores})\n",
    "df.to_csv('results/' + params['name'] + '_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Future Work\n",
    "\n",
    "There are a couple of things, I'd consider for future work. From a software engineering perspective, I'd be looking to generalize the configuration further to cover network architecture. At the moment the architecture is hard-coded, which does not invite to any hyperparameter tuning. Additionally, the network architecture could be extended to work with the pixels of the environment rather than a derived state representation. This would be more akin to the work on reaching super-human performance in Atari games lead by several researchers. The agent implementations could be further extended to on-policy approaches, such as SARSA. State of the art performance has been reached since with an agent that combines a number of different approaches into a rainbow implementation [Hessel et al.](https://arxiv.org/abs/1710.02298).\n",
    "\n",
    "However, before considering any of these extensions, I'd prioritize a more rigorous experimental setup with a proper experimental design and hyperparameter tuning to find the best achieving agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
